{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementation\n",
    "\n",
    "## Data preparation\n",
    "\n",
    "For this tutorial we will use bilingual datasets from [tatoeba.org](https://tatoeba.org/eng/downloads).\n",
    "You can download language pairs data from http://www.manythings.org/anki/, or if there is no pair,\n",
    "as for Ukrainian-German, you can use my script [get_dataset.py](https://github.com/tsdaemon/neural-experiments/blob/master/nmt/scripts/get_dataset.py). It will download raw data from\n",
    "tatoeba and extract a bilingual dataset as a `csv` file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "source_lang = 'ukr'\n",
    "target_lang = 'deu'\n",
    "data_dir = '../../neural-experiments/nmt/data/'\n",
    "\n",
    "corpus = pd.read_csv(os.path.join(data_dir, '{}-{}.csv'.format(source_lang, target_lang)), delimiter='\\t')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To train neural network we need to turn the sentences into something the neural network can understand, which of course means numbers. Each sentence will be split into words and turned into a sequence of numbers. To do this, we will use a vocabulary – a class which will store word indexes.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "SOS_token = '<start>'\n",
    "EOS_token = '<end>'\n",
    "UNK_token = '<unk>'\n",
    "PAD_token = '<pad>'\n",
    "\n",
    "SOS_idx = 0\n",
    "EOS_idx = 1\n",
    "UNK_idx = 2\n",
    "PAD_idx = 3\n",
    "\n",
    "class Vocab:\n",
    "    def __init__(self):\n",
    "        self.index2word = {\n",
    "            SOS_idx: SOS_token,\n",
    "            EOS_idx: EOS_token,\n",
    "            UNK_idx: UNK_token,\n",
    "            PAD_idx: PAD_token\n",
    "        }\n",
    "        self.word2index = {v: k for k, v in self.index2word.items()}\n",
    "\n",
    "    def index_words(self, words):\n",
    "        for word in words:\n",
    "            self.index_word(word)\n",
    "\n",
    "    def index_word(self, word):\n",
    "        if word not in self.word2index:\n",
    "            n_words = len(self)\n",
    "            self.word2index[word] = n_words\n",
    "            self.index2word[n_words] = word\n",
    "\n",
    "    def __len__(self):\n",
    "        assert len(self.index2word) == len(self.word2index)\n",
    "        return len(self.index2word)\n",
    "    \n",
    "    def unidex_words(self, indices):\n",
    "        return [self.index2word[i] for i in indices]\n",
    "    \n",
    "    def to_file(self, filename):\n",
    "        values = [w for w, k in sorted(list(self.word2index.items())[5:])]\n",
    "        with open(filename, 'w') as f:\n",
    "            f.write('\\n'.join(values))\n",
    "\n",
    "    @classmethod\n",
    "    def from_file(cls, filename):\n",
    "        vocab = Vocab()\n",
    "        with open(filename, 'r') as f:\n",
    "            words = [l.strip() for l in f.readlines()]\n",
    "            vocab.index_words(words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define tokenizer functions for you languages to split sentences on words.\n",
    "In this examples I'm using standard [`nltk.tokenize.WordPunctTokenizer`](https://kite.com/python/docs/nltk.tokenize.WordPunctTokenizer) for German and\n",
    "a function `tokenize_words` from package [`tokenize_uk`](https://github.com/lang-uk/tokenize-uk) for Ukrainian.\n",
    "Also, replace input file with your\n",
    "\n",
    "Since there are a lot of example sentences and we want to train something quickly, we'll trim the data set to only relatively short and simple sentences. Here the maximum length is 8 words (that includes punctuation) and we're filtering to sentences that translate to the form \"I am\" or \"He is\" etc.\n",
    "\n",
    "Additionaly, you might want to filter out rare words which occur only few times in corpus. This words could not be learned efficiently since there are not enough training examples for them. This will reduce vocabulary size and decrease training time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training corpus length: 11582\n",
      "Source vocabulary size: 4131\n",
      "Target vocabulary size: 3201\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from tokenize_uk import tokenize_words\n",
    "import pandas as pd\n",
    "\n",
    "max_length = 8\n",
    "min_word_count = 2\n",
    "\n",
    "tokenizers = {\n",
    "    'ukr': tokenize_words,\n",
    "    'deu': nltk.tokenize.WordPunctTokenizer().tokenize\n",
    "}\n",
    "\n",
    "def preprocess_corpus(sents, tokenizer, min_word_count):\n",
    "    n_words = {}\n",
    "\n",
    "    sents_tokenized = []\n",
    "    for sent in sents:\n",
    "        sent_tokenized = [w.lower() for w in tokenizer(sent)]\n",
    "\n",
    "        sents_tokenized.append(sent_tokenized)\n",
    "\n",
    "        for word in sent_tokenized:\n",
    "            if word in n_words:\n",
    "                n_words[word] += 1\n",
    "            else:\n",
    "                n_words[word] = 1\n",
    "\n",
    "    for i, sent_tokenized in enumerate(sents_tokenized):\n",
    "        sent_tokenized = [t if n_words[t] >= min_word_count else UNK_token for t in sent_tokenized]\n",
    "        sents_tokenized[i] = sent_tokenized\n",
    "\n",
    "    return sents_tokenized\n",
    "\n",
    "def read_vocab(sents):\n",
    "    vocab = Vocab()\n",
    "    for sent in sents:\n",
    "        vocab.index_words(sent)\n",
    "\n",
    "    return vocab\n",
    "\n",
    "source_sents = preprocess_corpus(corpus['text' + source_lang], tokenizers[source_lang], min_word_count)\n",
    "target_sents = preprocess_corpus(corpus['text' + target_lang], tokenizers[target_lang], min_word_count)\n",
    "\n",
    "source_sents, target_sents = zip(\n",
    "    *[(s, t) for s, t in zip(source_sents, target_sents)\n",
    "      if len(s) < max_length and len(t) < max_length]\n",
    ")\n",
    "\n",
    "source_vocab = read_vocab(source_sents)\n",
    "target_vocab = read_vocab(target_sents)\n",
    "target_vocab.to_file(os.path.join(data_dir, '{}.vocab.txt'.format(target_lang)))\n",
    "source_vocab.to_file(os.path.join(data_dir, '{}.vocab.txt'.format(source_lang)))\n",
    "\n",
    "print('Training corpus length: {}\\nSource vocabulary size: {}\\nTarget vocabulary size: {}'.format(\n",
    "    len(source_sents), len(source_vocab.word2index), len(target_vocab.word2index)\n",
    "))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data for deep learning experiment is usually split on tree parts: \n",
    "* Training data is used for neural network training;\n",
    "* Development data is used to select optiomal training stop point;\n",
    "* Test data is used for final evaluation of experiment performance.\n",
    "\n",
    "We will use 80% of data as a training set, 6% of data as a development set and 14% of data as a test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "RANDOM_SEED = 42\n",
    "np.random.seed(RANDOM_SEED)\n",
    "\n",
    "source_length = len(source_sents)\n",
    "inidices = np.random.permutation(source_length)\n",
    "\n",
    "training_indices = inidices[:int(x_length*0.8)]\n",
    "dev_indices = inidices[int(x_length*0.8):int(x_length*0.86)]\n",
    "test_indices = inidices[int(x_length*0.86):]\n",
    "\n",
    "training_source = [source_sents[i] for i in training_indices]\n",
    "dev_source = [source_sents[i] for i in dev_indices]\n",
    "test_source = [source_sents[i] for i in test_indices]\n",
    "\n",
    "training_target = [target_sents[i] for i in training_indices]\n",
    "dev_target = [target_sents[i] for i in dev_indices]\n",
    "test_target = [target_sents[i] for i in test_indices]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PyTorch uses it's own format of data – Tensor. A Tensor is a multi-dimensional array of numbers with some type e.g. FloatTensor or LongTensor. Before we can use our training data, we need to convert it into tensors using previously defined word indices.\n",
    "Additionaly, we need to add special tokens SOS (start of sentence) and EOS (end of sentence) to each sentence.\n",
    "Also, all sentences should have the same length to make possible batch training, therefore we will extend them with token PAD if needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "def indexes_from_sentence(vocab, sentence):\n",
    "    return [vocab.word2index[word] for word in sentence]\n",
    "\n",
    "def tensor_from_sentence(vocab, sentence, max_seq_length):\n",
    "    indexes = indexes_from_sentence(vocab, sentence)\n",
    "    indexes.append(EOS_idx)\n",
    "    indexes.insert(0, SOS_idx)\n",
    "    # we need to have all sequences the same length to process them in batches\n",
    "    if len(indexes) < max_seq_length:\n",
    "        indexes += [PAD_idx] * (max_seq_length - len(indexes))\n",
    "    tensor = torch.LongTensor(indexes)\n",
    "    return tensor\n",
    "\n",
    "def tensors_from_pair(source_sent, target_sent, max_seq_length):\n",
    "    source_tensor = tensor_from_sentence(source_vocab, source_sent, max_seq_length).unsqueeze(1)\n",
    "    target_tensor = tensor_from_sentence(target_vocab, target_sent, max_seq_length).unsqueeze(1)\n",
    "    return (source_tensor, target_tensor)\n",
    "\n",
    "max_seq_length = max_length + 2  # 2 for EOS_token and SOS_token\n",
    "\n",
    "training = []\n",
    "for source_sent, target_sent in zip(training_source, training_target):\n",
    "    training.append(tensors_from_pair(source_sent, target_sent, max_seq_length))\n",
    "    \n",
    "x_training, y_training = zip(*training)\n",
    "x_training = torch.transpose(torch.cat(x_training, dim=-1), 1, 0)\n",
    "y_training = torch.transpose(torch.cat(y_training, dim=-1), 1, 0)\n",
    "torch.save(x_training, os.path.join(data_dir, 'x_training.bin'))\n",
    "torch.save(y_training, os.path.join(data_dir, 'y_training.bin'))\n",
    "\n",
    "x_development = []\n",
    "for source_sent in dev_source:\n",
    "    tensor = tensor_from_sentence(source_vocab, source_sent, max_seq_length).unsqueeze(1)\n",
    "    x_development.append(tensor)\n",
    "    \n",
    "x_development = torch.transpose(torch.cat(x_development, dim=-1), 1, 0)\n",
    "torch.save(x_development, os.path.join(data_dir, 'x_development.bin'))\n",
    "\n",
    "x_test = []\n",
    "for source_sent in test_source:\n",
    "    tensor = tensor_from_sentence(source_vocab, source_sent, max_seq_length).unsqueeze(1)\n",
    "    x_test.append(tensor)\n",
    "    \n",
    "x_test = torch.transpose(torch.cat(x_test, dim=-1), 1, 0)\n",
    "torch.save(x_test, os.path.join(data_dir, 'x_test.bin'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encoder\n",
    "\n",
    "The encoder of a Seq2seq network is a [**Recurrent Neural Network**](https://en.wikipedia.org/wiki/Recurrent_neural_network). Recurrent network can process model a sequence of related data (sentence in our case) using the same set of weights. To do this, RNN uses it's output from a previous step as input along with input from the sequence. \n",
    "\n",
    "Naive implementation of RNN is subject to problems with a graient for long sequences; therefore, I use [**Long-Short Term Memory**](https://en.wikipedia.org/wiki/Long_short-term_memory) as recurrent module. You should not care about it's implementation since it already implemented in PyTorch: [nn.LSTM](https://pytorch.org/docs/stable/_modules/torch/nn/modules/rnn.html#LSTM). This module allows bi-directional sequence processing out-of-the-box – this allows to capture backward relations in sentence as well as forward relations.\n",
    "\n",
    "Additionally, I use embeddings module to convert word indices into dense vectors. This allow to project discreete symbols (words) into continuous space which reflects semantical relations in spatial words positions. For this experiment I will not use pretrained word vectors and train this representations using machine translation supervision signal. But you may use pretrained word embeddings (for Ukrainian [lang-uk](http://lang.org.ua/en/models/#anchor4) project).\n",
    "\n",
    "To not forget meaning of dimensions for input vectors, usually I leave comments like `# word_inputs: (batch_size, seq_length)`. Above means that variable `word_inputs` contains reference to tensor, which has shape `(batch_size, seq_length)`, e.g. it is an array of sequences of length `seq_length`, where array length is `batch_size`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.init as init\n",
    "\n",
    "USE_CUDA = False\n",
    "\n",
    "class EncoderRNN(nn.Module):\n",
    "    def __init__(self, vocab_size, hidden_size, n_layers=1):\n",
    "        super(EncoderRNN, self).__init__()\n",
    "        \n",
    "        self.vocab_size = vocab_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.n_layers = n_layers\n",
    "        \n",
    "        self.embedding = nn.Embedding(vocab_size, hidden_size)\n",
    "        init.normal_(self.embedding.weight, 0.0, 0.2)\n",
    "        \n",
    "        self.lstm = nn.LSTM(\n",
    "            hidden_size, \n",
    "            int(hidden_size/2),  # Bi-directional processing will ouput vectors of double size, therefore I reduced output dimensionality\n",
    "            num_layers=n_layers, \n",
    "            batch_first=True,  # First dimension of input tensor will be treated as a batch dimension\n",
    "            bidirectional=True\n",
    "        )\n",
    "        \n",
    "    # word_inputs: (batch_size, seq_length), h: (h_or_c, layer_n_direction, batch, seq_length)\n",
    "    def forward(self, word_inputs, hidden):         \n",
    "        # embedded (batch_size, seq_length, hidden_size)\n",
    "        embedded = self.embedding(word_inputs) \n",
    "        # output (batch_size, seq_length, hidden_size*directions)\n",
    "        # hidden (h: (batch_size, num_layers*directions, hidden_size),\n",
    "        #         c: (batch_size, num_layers*directions, hidden_size))\n",
    "        output, hidden = self.lstm(embedded, hidden)\n",
    "        return output, hidden\n",
    "\n",
    "    def init_hidden(self, batches):\n",
    "        hidden = torch.zeros(2, self.n_layers*2, batches, int(self.hidden_size/2))\n",
    "        if USE_CUDA: hidden = hidden.cuda()\n",
    "        return hidden"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decoder\n",
    "\n",
    "Decoder module is similar to encoder with difference in that it generates a sequence, therefore it will process inputs one by one; therefore it can not be bidirectional."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class DecoderRNN(nn.Module):\n",
    "    def __init__(self, vocab_size, hidden_size, n_layers=1):\n",
    "        super(DecoderRNN, self).__init__()\n",
    "        \n",
    "        self.vocab_size = vocab_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.n_layers = n_layers\n",
    "        \n",
    "        self.embedding = nn.Embedding(vocab_size, hidden_size)\n",
    "        init.normal_(self.embedding.weight, 0.0, 0.2)\n",
    "        \n",
    "        self.lstm = nn.LSTM(hidden_size, hidden_size, num_layers=n_layers, batch_first=True, bidirectional=False)\n",
    "        \n",
    "    def forward(self, word_inputs, hidden):\n",
    "        # Note: we run this one by one\n",
    "        # embedded (batch_size, 1, hidden_size)\n",
    "        embedded = self.embedding(word_inputs).unsqueeze_(1)\n",
    "        output, hidden = self.lstm(embedded, hidden)\n",
    "        return output, hidden"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To make sure the Encoder and Decoder model are working (and working together) we'll do a quick test with fake word inputs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EncoderRNN(\n",
      "  (embedding): Embedding(10, 10)\n",
      "  (lstm): LSTM(10, 5, num_layers=2, batch_first=True, bidirectional=True)\n",
      ")\n",
      "torch.Size([1, 3, 10]) torch.Size([4, 1, 5]) torch.Size([4, 1, 5])\n"
     ]
    }
   ],
   "source": [
    "vocab_size = 10\n",
    "hidden_dim = 10\n",
    "n_layers = 2\n",
    "\n",
    "encoder_test = EncoderRNN(vocab_size, hidden_dim, n_layers)\n",
    "print(encoder_test)\n",
    "\n",
    "# Recurrent network requires initial hidden state\n",
    "encoder_hidden = encoder_test.init_hidden(1)\n",
    "\n",
    "# Test input of size (1x3), one sequence of size 3\n",
    "word_input = torch.LongTensor([[1, 2, 3]])\n",
    "\n",
    "if USE_CUDA:\n",
    "    encoder_test.cuda()\n",
    "    word_input = word_input.cuda()\n",
    "\n",
    "encoder_outputs, encoder_hidden = encoder_test(word_input, encoder_hidden)\n",
    "\n",
    "# encoder_outputs: (batch_size, seq_length, hidden_size)\n",
    "# encoder_hidden[0, 1]: (n_layers*2, batch_size, hidden_size/2)\n",
    "print(encoder_outputs.shape, encoder_hidden[0].shape, encoder_hidden[1].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`encoder_hidden` is tuple for h and c components of LSTM hidden state. In PyTorch, tensors of LSTM hidden components have a following meaning of dimensions: \n",
    "* First dimension is n_layers*directions, meaning that if we have bidirectional network, then each layer will store two items in this direction;\n",
    "* Second dimension is batch dimension\n",
    "* Third dimension is a hidden vector itself\n",
    "\n",
    "Decoder uses single directional LSTM, therefore we need to reshape encoders h and c before sending them into decoder: concat all bi-directional vectors into single-direction vectors. This means, that each two vectors along `n_layers*directions` I combine into single vector, increasing size of hidden vector dimension in two times and decreasing size of the first dimension to `n_layers`, which is two."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DecoderRNN(\n",
      "  (embedding): Embedding(10, 10)\n",
      "  (lstm): LSTM(10, 10, num_layers=2, batch_first=True)\n",
      ")\n",
      "torch.Size([1, 1, 10]) torch.Size([2, 1, 10]) torch.Size([2, 1, 10])\n",
      "torch.Size([1, 1, 10]) torch.Size([2, 1, 10]) torch.Size([2, 1, 10])\n",
      "torch.Size([1, 1, 10]) torch.Size([2, 1, 10]) torch.Size([2, 1, 10])\n"
     ]
    }
   ],
   "source": [
    "decoder_test = DecoderRNN(vocab_size, hidden_dim, n_layers)\n",
    "print(decoder_test)\n",
    "\n",
    "word_inputs = torch.LongTensor([[1, 2, 3]])\n",
    "\n",
    "decoder_hidden_h = encoder_hidden[0].reshape(2, 1, 10)\n",
    "decoder_hidden_c = encoder_hidden[1].reshape(2, 1, 10)\n",
    "\n",
    "if USE_CUDA:\n",
    "    decoder_test.cuda()\n",
    "    word_inputs = word_inputs.cuda()\n",
    "\n",
    "for i in range(3):\n",
    "    input = word_inputs[:, i]\n",
    "    decoder_output, decoder_hidden = decoder_test(input, (decoder_hidden_h, decoder_hidden_c))\n",
    "    decoder_hidden_h, decoder_hidden_c = decoder_hidden\n",
    "    print(decoder_output.size(), decoder_hidden_h.size(), decoder_hidden_c.size())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Seq2seq\n",
    "\n",
    "Logic to coordinate this two modules I have stored in a high-level module `Seq2seq`: it takes care about Encoder-Decoder coordination, transformation of decoder results into word probability distribution. \n",
    "\n",
    "Also, this module implements two `forward` functions: one for training time and second is for inference. The difference between these two functions is that during training I am using training `y` values (target sentence words) as decoder input; this is called [Teacher Forcing](https://machinelearningmastery.com/teacher-forcing-for-recurrent-neural-networks/). Obviously, during inference I don't have `y` values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Seq2seq(nn.Module):\n",
    "    def __init__(self, input_vocab_size, output_vocab_size, hidden_size, n_layers):\n",
    "        super(Seq2seq, self).__init__()\n",
    "        \n",
    "        self.n_layers = n_layers\n",
    "        self.hidden_size = hidden_size\n",
    "        \n",
    "        self.encoder = EncoderRNN(input_vocab_size, hidden_size, self.n_layers)\n",
    "        self.decoder = DecoderRNN(output_vocab_size, hidden_size, self.n_layers)\n",
    "        \n",
    "        self.W = nn.Linear(hidden_size, output_vocab_size)\n",
    "        self.softmax = nn.Softmax()\n",
    "        \n",
    "    def _forward_encoder(self, x):\n",
    "        batch_size = x.shape[0]\n",
    "        init_hidden = self.encoder.init_hidden(batch_size)\n",
    "        encoder_outputs, encoder_hidden = self.encoder(x, init_hidden)\n",
    "        encoder_hidden_h, encoder_hidden_c = encoder_hidden\n",
    "        \n",
    "        decoder_hidden_h = encoder_hidden_h.reshape(self.n_layers, batch_size, self.hidden_size)\n",
    "        decoder_hidden_c = encoder_hidden_c.reshape(self.n_layers, batch_size, self.hidden_size)\n",
    "        return decoder_hidden_h, decoder_hidden_c\n",
    "    \n",
    "    def forward_train(self, x, y):\n",
    "        decoder_hidden_h, decoder_hidden_c = self._forward_encoder(x)\n",
    "        \n",
    "        H = []\n",
    "        for i in range(y.shape[1]):\n",
    "            input = y[:, i]\n",
    "            decoder_output, decoder_hidden = self.decoder(input, (decoder_hidden_h, decoder_hidden_c))\n",
    "            decoder_hidden_h, decoder_hidden_c = decoder_hidden\n",
    "            # h: (batch_size, vocab_size)\n",
    "            h = self.W(decoder_output.squeeze(1))\n",
    "            # h: (batch_size, vocab_size, 1)\n",
    "            H.append(h.unsqueeze(2))\n",
    "        \n",
    "        # H: (batch_size, vocab_size, seq_len)\n",
    "        return torch.cat(H, dim=2)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        decoder_hidden_h, decoder_hidden_c = self._forward_encoder(x)\n",
    "        \n",
    "        current_y = SOS_idx\n",
    "        result = [current_y]\n",
    "        counter = 0\n",
    "        while current_y != EOS_idx and counter < 100:\n",
    "            input = torch.tensor([current_y])\n",
    "            decoder_output, decoder_hidden = self.decoder(input, (decoder_hidden_h, decoder_hidden_c))\n",
    "            decoder_hidden_h, decoder_hidden_c = decoder_hidden\n",
    "            # h: (vocab_size)\n",
    "            h = self.W(decoder_output.squeeze(1)).squeeze(0)\n",
    "            y = self.softmax(h)\n",
    "            _, current_y = torch.max(y, dim=0)\n",
    "            current_y = current_y.item()\n",
    "            result.append(current_y)\n",
    "            counter += 1\n",
    "            \n",
    "        return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training\n",
    "\n",
    "To optimize neural network weights we need to have a model itself and optimizer. Model is already defined and optimizer is usually available in NN framework. I use [Adam](http://ruder.io/optimizing-gradient-descent/index.html#adam) from [torch.optim](https://pytorch.org/docs/stable/optim.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.optim import Adam\n",
    "\n",
    "model = Seq2seq(len(source_vocab), len(target_vocab), 300, 1)\n",
    "optim = Adam(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since neural network training is a computationally expensive process, it is better to a train neural network for multiple examples at once. Therefore we need to split our training data on minibatches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "def batch_generator(batch_indices, batch_size):\n",
    "    batches = math.ceil(len(batch_indices)/batch_size)\n",
    "    for i in range(batches):\n",
    "        batch_start = i*batch_size\n",
    "        batch_end = (i+1)*batch_size\n",
    "        if batch_end > len(batch_indices):\n",
    "            yield batch_indices[batch_start:]\n",
    "        else:\n",
    "            yield batch_indices[batch_start:batch_end]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Previously I mentioned log-lieklyhood function which is used to optimize model parameters; in PyTorch this function is implemented in module `CrossEntropyLoss`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cross_entropy = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we can start to train our model. Each training epoch include forward propagation, which yields some training results for training target sentences; then `cross_entropy` loss is calculated and `loss.backward()` calculates gradient with respect to loss for each model parameter. After that, `optim.step()` uses gradient to adjust model parameters and minimize loss.\n",
    "\n",
    "After each training epoch, development set is used to evaluate model perfomance with [BLEU](https://en.wikipedia.org/wiki/BLEU) score. I use `early_stop_counter` to stop training process, if BLEU is not improving for 10 epochs. \n",
    "\n",
    "Module `tqdm` is optional to use, it is a handy and simple way to create progress bar for a comparably long operation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "754c748d24d642e8afaf457d31c9b853",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "A Jupyter Widget"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 training finished, loss: 2.567095502563145\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "435ba57b2b3b43c0981f9313d99eb9c9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "A Jupyter Widget"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/anatolii.stehnii/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:52: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "/Users/anatolii.stehnii/anaconda3/lib/python3.6/site-packages/nltk/translate/bleu_score.py:503: UserWarning: \n",
      "The hypothesis contains 0 counts of 2-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n",
      "/Users/anatolii.stehnii/anaconda3/lib/python3.6/site-packages/nltk/translate/bleu_score.py:503: UserWarning: \n",
      "The hypothesis contains 0 counts of 3-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n",
      "/Users/anatolii.stehnii/anaconda3/lib/python3.6/site-packages/nltk/translate/bleu_score.py:503: UserWarning: \n",
      "The hypothesis contains 0 counts of 4-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 validation finished, BLEU: 2.608244773227257e-80\n",
      "Translation example: ref \"sie ist gerade zwanzig geworden .\", hypothesis \"ich habe einen <unk> .\"\n",
      "The best model found, resetting eraly stop counter.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c5260dbe84c0405c99487fac68ce4cde",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "A Jupyter Widget"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 training finished, loss: 2.432845706525056\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d0e90b018365439cab587be316499e40",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "A Jupyter Widget"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 validation finished, BLEU: 2.608244773227257e-80\n",
      "Translation example: ref \"sie ist gerade zwanzig geworden .\", hypothesis \"ich habe einen <unk> .\"\n",
      "The best model found, resetting eraly stop counter.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f33f724acd5e44c98b74eb1781bb420a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "A Jupyter Widget"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 training finished, loss: 2.3167244144107983\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f293f31bae9b46209321c93a756c8fbb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "A Jupyter Widget"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 validation finished, BLEU: 2.608244773227257e-80\n",
      "Translation example: ref \"sie ist gerade zwanzig geworden .\", hypothesis \"ich habe einen <unk> .\"\n",
      "The best model found, resetting eraly stop counter.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8965511b87754268b3b7d79a934b012a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "A Jupyter Widget"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 training finished, loss: 2.215837465680164\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1e5b5ce12f61470d87cf9c846f2bd061",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "A Jupyter Widget"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 validation finished, BLEU: 3.447931236302247e-80\n",
      "Translation example: ref \"sie ist gerade zwanzig geworden .\", hypothesis \"ich habe ein <unk> .\"\n",
      "The best model found, resetting eraly stop counter.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "381ad8d1abf848bbbcb5e27b7bf90e08",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "A Jupyter Widget"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5 training finished, loss: 2.1276842057704926\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "203f967c732e4f9f928f7419a1402128",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "A Jupyter Widget"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5 validation finished, BLEU: 4.159747972100934e-80\n",
      "Translation example: ref \"sie ist gerade zwanzig geworden .\", hypothesis \"ich habe ein <unk> .\"\n",
      "The best model found, resetting eraly stop counter.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7b44e2b4f1634dccb1912702ecfd053d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "A Jupyter Widget"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6 training finished, loss: 2.0458891637947247\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "181814eb190e4d5a9dd1be399643c7a9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "A Jupyter Widget"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6 validation finished, BLEU: 2.608244773227257e-80\n",
      "Translation example: ref \"sie ist gerade zwanzig geworden .\", hypothesis \"ich habe einen <unk> .\"\n",
      "The best model found, resetting eraly stop counter.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6a0727c2a2ae45279dbdf4e7f4372141",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "A Jupyter Widget"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7 training finished, loss: 1.9656824404778688\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b0ce64152f4f4a05be89c961889c9d4f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "A Jupyter Widget"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7 validation finished, BLEU: 2.608244773227257e-80\n",
      "Translation example: ref \"sie ist gerade zwanzig geworden .\", hypothesis \"ich habe einen <unk> .\"\n",
      "The best model found, resetting eraly stop counter.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8414c73d0a2841049206d4c701290f9d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "A Jupyter Widget"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8 training finished, loss: 1.890369696461636\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bdf7ec1155c74c94abb12f3a7d4293b0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "A Jupyter Widget"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8 validation finished, BLEU: 0.004316546762589928\n",
      "Translation example: ref \"sie ist gerade zwanzig geworden .\", hypothesis \"ich bin <unk> .\"\n",
      "The best model found, resetting eraly stop counter.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "147e168997ca4142927f18631c158ffe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "A Jupyter Widget"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9 training finished, loss: 1.8211928022944408\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4d1b7c2570ef4750ba2413a256514c44",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "A Jupyter Widget"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9 validation finished, BLEU: 0.004316546762589928\n",
      "Translation example: ref \"sie ist gerade zwanzig geworden .\", hypothesis \"ich bin <unk> .\"\n",
      "The best model found, resetting eraly stop counter.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5751ecddc88c42c49f404810dd587ac6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "A Jupyter Widget"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10 training finished, loss: 1.7584543746450674\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "04e4bbed68ae485083beed12cceedfdb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "A Jupyter Widget"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10 validation finished, BLEU: 8.903444312511448e-156\n",
      "Translation example: ref \"sie ist gerade zwanzig geworden .\", hypothesis \"ich habe einen <unk> .\"\n",
      "The best model found, resetting eraly stop counter.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ddb90876418c4de299141952aea43499",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "A Jupyter Widget"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11 training finished, loss: 1.6997281675753386\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f5d47dd753b6409cac53486cc5b76fe3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "A Jupyter Widget"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11 validation finished, BLEU: 2.608244773227257e-80\n",
      "Translation example: ref \"sie ist gerade zwanzig geworden .\", hypothesis \"ich habe einen <unk> .\"\n",
      "The best model found, resetting eraly stop counter.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cc7a35be38184bf69a07146d07b2e085",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "A Jupyter Widget"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12 training finished, loss: 1.6408083905344424\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0c9a5010d4894bfd895cd06995d5ad09",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "A Jupyter Widget"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12 validation finished, BLEU: 1.7388298488181715e-80\n",
      "Translation example: ref \"sie ist gerade zwanzig geworden .\", hypothesis \"ich habe einen <unk> .\"\n",
      "The best model found, resetting eraly stop counter.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ef19aa2f4362420f99bc8d12cac565a3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "A Jupyter Widget"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13 training finished, loss: 1.5846104971740558\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fb20c631b9c9419497fc9c3d9b43e4ac",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "A Jupyter Widget"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13 validation finished, BLEU: 8.694149244090858e-81\n",
      "Translation example: ref \"sie ist gerade zwanzig geworden .\", hypothesis \"ich habe ihn nicht vergessen .\"\n",
      "The best model found, resetting eraly stop counter.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "63d9fa71c6844c6e9199f3d50c8c709b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "A Jupyter Widget"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14 training finished, loss: 1.5252591423366382\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eca42ec2902745d19564fa0151b9a94d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "A Jupyter Widget"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14 validation finished, BLEU: 0.004316546762589928\n",
      "Translation example: ref \"sie ist gerade zwanzig geworden .\", hypothesis \"ich habe einen <unk> .\"\n",
      "The best model found, resetting eraly stop counter.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d84cd6f007f6417394e69cd6082b52be",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "A Jupyter Widget"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 15 training finished, loss: 1.4653565417165342\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bb26bfcf691947cdaaa9bf2b88c36421",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "A Jupyter Widget"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 15 validation finished, BLEU: 8.694149244090858e-81\n",
      "Translation example: ref \"sie ist gerade zwanzig geworden .\", hypothesis \"ich habe einen computer .\"\n",
      "The best model found, resetting eraly stop counter.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e98ddc92349f4141820abf2e138cdd38",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "A Jupyter Widget"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 16 training finished, loss: 1.4076407474020254\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "37c379a126f34bb292cb111239d452d5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "A Jupyter Widget"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 16 validation finished, BLEU: 8.694149244090858e-81\n",
      "Translation example: ref \"sie ist gerade zwanzig geworden .\", hypothesis \"ich habe keinen computer .\"\n",
      "The best model found, resetting eraly stop counter.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9ad25645274240c1a3d1366047bff866",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "A Jupyter Widget"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 17 training finished, loss: 1.3506482684093972\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "88e5859481c04ebb9c53de42fc391844",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "A Jupyter Widget"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 17 validation finished, BLEU: 4.967174974064844e-156\n",
      "Translation example: ref \"sie ist gerade zwanzig geworden .\", hypothesis \"ich bin <unk> .\"\n",
      "The best model found, resetting eraly stop counter.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "936c8e79aad240e0811155eab822941f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "A Jupyter Widget"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 18 training finished, loss: 1.2917404511700505\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d7d1ac86491749e4beabcc779ecda6ee",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "A Jupyter Widget"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 18 validation finished, BLEU: 5.632517020724474e-156\n",
      "Translation example: ref \"sie ist gerade zwanzig geworden .\", hypothesis \"ich bin <unk> .\"\n",
      "The best model found, resetting eraly stop counter.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "432bc885b1b54fe6b2b440494e5cb3d3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "A Jupyter Widget"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 19 training finished, loss: 1.2335404539885728\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f17a1aab7b2542a48f79b16a3a98654b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "A Jupyter Widget"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 19 validation finished, BLEU: 5.616200269341641e-156\n",
      "Translation example: ref \"sie ist gerade zwanzig geworden .\", hypothesis \"ich bin nicht schüchtern .\"\n",
      "The best model found, resetting eraly stop counter.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "db9b4e80fabe463ab029bf8deafe9da3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "A Jupyter Widget"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 20 training finished, loss: 1.1747383365164632\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "077d5c43d8634509888f634beb101a93",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "A Jupyter Widget"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 20 validation finished, BLEU: 5.42437303649243e-156\n",
      "Translation example: ref \"sie ist gerade zwanzig geworden .\", hypothesis \"ich bin ein unabhängiger mensch .\"\n",
      "The best model found, resetting eraly stop counter.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ddcc33444094466c98a4077e51b0f65c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "A Jupyter Widget"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-147-5357cdd588d2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m         \u001b[0moptim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m         \u001b[0moptim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/anatolii.stehnii/anaconda3/lib/python3.6/site-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m     91\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m         \"\"\"\n\u001b[0;32m---> 93\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     94\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/anatolii.stehnii/anaconda3/lib/python3.6/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m     87\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m     88\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 89\u001b[0;31m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m     90\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     91\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm_notebook as tqdm\n",
    "from nltk.translate.bleu_score import sentence_bleu\n",
    "\n",
    "BATCH_SIZE = 100\n",
    "total_batches = int(len(x_training)/BATCH_SIZE)\n",
    "early_stop_after = 10\n",
    "early_stop_counter = 0\n",
    "\n",
    "best_bleu = 0.0\n",
    "\n",
    "for epoch in range(10000):\n",
    "    total_loss = 0\n",
    "    for batch in tqdm(batch_generator(list(range(len(x_training))), BATCH_SIZE), \n",
    "                      desc='Training epoch {}'.format(epoch+1), \n",
    "                      total=total_batches):\n",
    "        x = x_training[batch, :] \n",
    "        # y for teacher forcing is all sequence without a last element \n",
    "        y_tf = y_training[batch, :-1] \n",
    "        # y for loss calculation is all sequence without a last element \n",
    "        y_true = y_training[batch, 1:]\n",
    "        # (batch_size, vocab_size, seq_length)\n",
    "        H = model.forward_train(x, y_tf)\n",
    "        loss = cross_entropy(H, y_true)\n",
    "        assert loss.item() > 0\n",
    "        \n",
    "        optim.zero_grad()\n",
    "        loss.backward()\n",
    "        optim.step()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "    print('Epoch {} training finished, loss: {}'.format(epoch+1, total_loss/total_batches))\n",
    "    \n",
    "    bleu = 0.0\n",
    "    dev_length = len(dev_target)\n",
    "    for x, reference in tqdm(zip(x_development, dev_target), \n",
    "                             desc='Validating epoch {}'.format(epoch+1), \n",
    "                             total=dev_length):\n",
    "        y = model(x.unsqueeze(0))\n",
    "        hypothesis = target_vocab.unidex_words(y[1:-1])  # Remove SOS and EOS\n",
    "        bleu += sentence_bleu([reference], hypothesis)\n",
    "        \n",
    "    bleu /= dev_length\n",
    "    source = ' '.join(source_vocab.unidex_words(x.tolist()[0]))\n",
    "    print ('Epoch {} validation finished, BLEU: {}\\nTranslation example: source \"{}\", ref \"{}\", hypothesis \"{}\"'.format(\n",
    "        epoch+1, bleu, source, ' '.join(reference), ' '.join(hypothesis)\n",
    "    ))\n",
    "    \n",
    "    if bleu > best_bleu:\n",
    "        early_stop_counter = 0\n",
    "        print('The best model found, resetting eraly stop counter.')\n",
    "        best_bleu = bleu\n",
    "    else:\n",
    "        early_stop_counter += 1\n",
    "        print('No improvement, early stop counter: {}.'.fromat(early_stop_counter))\n",
    "        if early_stop_counter >= early_stop_after:\n",
    "            print('Early stop!')\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
