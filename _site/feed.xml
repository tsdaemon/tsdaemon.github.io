<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="3.6.2">Jekyll</generator><link href="http://localhost:4000/feed.xml" rel="self" type="application/atom+xml" /><link href="http://localhost:4000/" rel="alternate" type="text/html" /><updated>2018-07-14T21:22:51+03:00</updated><id>http://localhost:4000/</id><title type="html">Daemon Engineer blog</title><subtitle>Science and education enthusiast blog. Stories and tutorials about deep learning, reinforcement learning, natural language processing, data visualization and software development.</subtitle><entry><title type="html">Neural Machine Translation With PyTorch. Tutorial 1: Encoder-decoder</title><link href="http://localhost:4000/2018/07/08/nmt-with-pytorch-seq-to-seq.html" rel="alternate" type="text/html" title="Neural Machine Translation With PyTorch. Tutorial 1: Encoder-decoder" /><published>2018-07-08T15:37:21+03:00</published><updated>2018-07-08T15:37:21+03:00</updated><id>http://localhost:4000/2018/07/08/nmt-with-pytorch-seq-to-seq</id><content type="html" xml:base="http://localhost:4000/2018/07/08/nmt-with-pytorch-seq-to-seq.html">&lt;p&gt;&lt;strong&gt;Recently I did a &lt;a href=&quot;https://github.com/tsdaemon/dl-nlp-2018&quot;&gt;3-day workshop about Deep Learning for Natural Language Processing&lt;/a&gt;.
Clearly 3 days was not enough to cover all topics in this broad field, therefore
I decided to create a series of practical tutorials about Neural Machine Translation
in &lt;a href=&quot;https://pytorch.org/&quot;&gt;PyTorch&lt;/a&gt;.&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;em&gt;I will start with a simple neural translation model and gradually improve it using
modern neural methods and techniques. In this first tutorial, I am going to create
Ukrainian-German translator with an encoder-decoder model.&lt;/em&gt;&lt;/p&gt;

&lt;h1 id=&quot;introduction&quot;&gt;Introduction&lt;/h1&gt;

&lt;p&gt;For almost 25 years, mainstream translation systems used &lt;a href=&quot;https://en.wikipedia.org/wiki/Statistical_machine_translation&quot;&gt;Statistical Machine Translation&lt;/a&gt;. SMT
methods were not outperformed until 2016, when Google AI released &lt;a href=&quot;https://ai.googleblog.com/2016/09/a-neural-network-for-machine.html&quot;&gt;results of their Neural Machine Translation model&lt;/a&gt;
and started to use it in Google Translation for 9 languages.&lt;/p&gt;

&lt;p&gt;Important to notice, that GMNT result was mostly defined by a huge amount of tranining data and
extensive computational power, which makes impossible to reproduce this results for
individual researchers. However, ideas and techniques, which were used in this architecture,
were reused to solve many other problems: question answering, natural database interface,
speech-to-text and text-to-speech and so on. Therefore, any deep learning expert
can benefit from understanding of how modern NMT works.&lt;/p&gt;

&lt;h1 id=&quot;theory&quot;&gt;Theory&lt;/h1&gt;

&lt;p&gt;Even though I am mostly practitioner but I still prefer to have a solid
mathematical representation of any model I am working with. This allows to maintain
a correct abstract understanding of problem which my model is solving. Therefore I will
start with formulation of NMT problem:&lt;/p&gt;

&lt;p&gt;Given a sentence in source language $\textbf{x}$, the task is to infer a
  sentence in target language $\textbf{y}$ which maximizes conditional
  probability $p(\textbf{y}|\textbf{x})$:&lt;/p&gt;

&lt;p&gt;\begin{equation}
  \textbf{y} = \underset{\hat{\textbf{y}}}{\mathrm{argmax}} p(\hat{\textbf{y}}|\textbf{x})
  \end{equation}&lt;/p&gt;</content><author><name>Anatolii Stehnii</name></author><category term="nmt," /><category term="dl," /><category term="PyTorch" /><summary type="html">Recently I did a 3-day workshop about Deep Learning for Natural Language Processing. Clearly 3 days was not enough to cover all topics in this broad field, therefore I decided to create a series of practical tutorials about Neural Machine Translation in PyTorch.</summary></entry></feed>